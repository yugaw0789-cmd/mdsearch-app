import os, sqlite3, numpy as np, re, datetime
import streamlit as st
from openai import OpenAI

DB_PATH_DEFAULT = "mdindex.sqlite"
EXPORT_DIR = "./exports"
EMBED_MODEL = "text-embedding-3-small"
ANSWER_MODEL = "gpt-4o"

# ---------- DB helpers ----------
@st.cache_resource
def get_conn(db_path: str):
    con = sqlite3.connect(db_path, check_same_thread=False)
    con.row_factory = sqlite3.Row
    return con

def exact_fts(con, query: str, k: int, phrase: bool):
    match = f'"{query}"' if phrase else query
    sql = """
    SELECT p.id, p.file, p.section, p.start, p.text,
           snippet(passages_fts,-1,'[',']',' â€¦ ',32) AS snippet
    FROM passages_fts
    JOIN passages p ON passages_fts.rowid = p.id
    WHERE passages_fts MATCH ?
    LIMIT ?
    """
    cur = con.cursor()
    cur.execute(sql, (match, k))
    return cur.fetchall()

def substring_like(con, query: str, k: int):
    sql = """
    SELECT id, file, section, substr(text, max(1, instr(text, ?)-40), 160) AS snippet
    FROM passages
    WHERE text LIKE ?
    LIMIT ?
    """
    cur = con.cursor()
    cur.execute(sql, (query, f"%{query}%", k))
    return cur.fetchall()

def fetch_vectors(con):
    cur = con.cursor()
    cur.execute("SELECT passage_id, dim, vec FROM vectors")
    rows = cur.fetchall()
    ids, vecs = [], []
    for pid, dim, blob in rows:
        v = np.frombuffer(blob, dtype=np.float32)
        if v.shape[0] == dim:
            ids.append(pid)
            vecs.append(v)
    return (np.vstack(vecs) if vecs else np.zeros((0,1536),dtype=np.float32)), ids

def fetch_passages_by_ids(con, ids):
    if not ids:
        return []
    cur = con.cursor()
    q = "SELECT id, file, section, text FROM passages WHERE id IN (%s)" % ",".join("?"*len(ids))
    cur.execute(q, ids)
    rows = {r["id"]: r for r in cur.fetchall()}
    return [rows[i] for i in ids if i in rows]

def fetch_section_text(con, file: str, section: str) -> str:
    cur = con.cursor()
    cur.execute(
        """
        SELECT text FROM passages
        WHERE file = ? AND section = ?
        ORDER BY start ASC
        """,
        (file, section),
    )
    rows = cur.fetchall()
    return "\n\n".join(r[0] for r in rows) if rows else ""

# ---------- è¦‹å‡ºã—è©•ä¾¡ï¼ˆå„ªå…ˆåº¦ä»˜ã‘ï¼‰ ----------
_heading_pat = re.compile(r"^(#+)\s*(.+?)\s*$")

def parse_section(section: str):
    m = _heading_pat.match(section.strip())
    if not m:
        return 6, section.strip()
    level = len(m.group(1))  # #æ•°ã€‚å°‘ãªã„ã»ã©ä¸Šä½
    title = m.group(2).strip()
    return level, title

def section_rank(section: str, query: str) -> float:
    level, title = parse_section(section)
    q = query.lower()
    t = title.lower()
    hit = (q in t)
    starts = t.startswith(q)
    exact = (t == q)
    score = 0.0
    if hit:    score += 50
    if starts: score += 20
    if exact:  score += 50
    score += max(0, 30 - 5*max(1, level))  # éšŽå±¤æµ…ã„ã»ã©åŠ ç‚¹
    return score

def ranked_exact_hits(con, query: str, k: int, phrase: bool, pool_factor: int = 4):
    pool = exact_fts(con, query, k*pool_factor, phrase)
    uniq = {}
    for r in pool:
        key = (r["file"], r["section"])
        if key not in uniq:
            uniq[key] = r
    rows = list(uniq.values())
    rows.sort(key=lambda r: section_rank(r["section"], query), reverse=True)
    return rows[:k]

# ãƒ’ãƒƒãƒˆä½ç½®ã‹ã‚‰æ¬¡ã® ### ç›´å‰ã¾ã§ã‚’ãƒ•ã‚¡ã‚¤ãƒ«æ¨ªæ–­ã§é€£çµï¼ˆé€£ç¶šãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
def collect_until_next_heading_across_files(
    con, start_file: str, start_offset: int, max_files: int = 5, max_chars: int = 0
) -> str:
    cur = con.cursor()
    cur.execute("SELECT DISTINCT file FROM passages ORDER BY file ASC")
    files = [r[0] for r in cur.fetchall()]
    try:
        i0 = files.index(start_file)
    except ValueError:
        return ""

    h_re = re.compile(r"(?m)^\s*###\s+")
    fence_re = re.compile(r"(?s)```.*?```")

    out_parts, total = [], 0
    files_used, stop, first_chunk_handled = 0, False, False

    for fi in range(i0, len(files)):
        if files_used >= max_files or stop:
            break
        f = files[fi]

        if fi == i0:
            cur.execute(
                "SELECT start, text FROM passages WHERE file=? AND start>=? ORDER BY start ASC",
                (f, start_offset),
            )
        else:
            cur.execute("SELECT start, text FROM passages WHERE file=? ORDER BY start ASC", (f,))
        rows = cur.fetchall()

        for start, text in rows:
            if fi == i0 and start_offset > start:
                cut = start_offset - start
                if 0 < cut < len(text):
                    text = text[cut:]
                elif cut >= len(text):
                    continue

            masked = fence_re.sub(lambda m: " " * (m.end() - m.start()), text)
            search_from = 0
            if not first_chunk_handled:
                m0 = h_re.match(masked)
                if m0:
                    search_from = m0.end()  # å…ˆé ­ã® ### ã¯ã‚¹ã‚­ãƒƒãƒ—
                first_chunk_handled = True

            m = h_re.search(masked, search_from)
            if m:
                seg = text[: m.start()]
                out_parts.append(seg)
                total += len(seg)
                stop = True
                break
            else:
                out_parts.append(text)
                total += len(text)

            if max_chars and total >= max_chars:
                return "".join(out_parts)[:max_chars]

        files_used += 1

    joined = "".join(out_parts)
    if max_chars and len(joined) > max_chars:
        joined = joined[:max_chars]
    return joined

# ---------- OpenAI helpers ----------
@st.cache_resource
def get_client():
    return OpenAI()

def embed(client: OpenAI, texts):
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return np.vstack([np.array(d.embedding, dtype=np.float32) for d in resp.data])

def ai_summarize_from_exact(client: OpenAI, query: str, full_text: str, user_prompt: str, temperature: float = 0.2):
    system = (
        "ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ã‚’æ­£ç¢ºã«æ•´ç†ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        "å…¥åŠ›ã•ã‚ŒãŸåŽŸæ–‡ã‹ã‚‰ã®ã¿æƒ…å ±ã‚’æŠœãå‡ºã—ã€æŽ¨æ¸¬ã¯æœ€å°é™ã«ã™ã‚‹ã€‚"
        "å‡ºåŠ›ã¯Markdownã§ã€å¿…ãšä»¥ä¸‹ã®é †åºã§æ§‹æˆ:\n"
        "## çµè«–\n- è¦ç‚¹ã‚’ç°¡æ½”ã«\n"
        "## è©³ç´°æ•´ç†\n- è¡¨ã‚„å°è¦‹å‡ºã—ã§è«–ç†çš„ã«ã¾ã¨ã‚ã‚‹\n"
        "## æ³¨æ„ãƒ»ä¸æ˜Žç‚¹\n- åŽŸæ–‡ã«ç„¡ã„äº‹é …ã¯ã€ŽåŽŸæ–‡ã«è¨˜è¼‰ãªã—ã€ã¨æ˜Žè¨˜"
    )
    user = f"æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰: {query}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{user_prompt}\n\n[åŽŸæ–‡]\n{full_text}"
    resp = client.chat.completions.create(
        model=ANSWER_MODEL,
        messages=[{"role":"system","content":system},{"role":"user","content":user}],
        temperature=temperature,
    )
    return resp.choices[0].message.content

# ---------- äººæ‰‹ç”¨ï¼šæ ¹æ‹ é¸æŠžUIã®ãŸã‚ã®æ•´å½¢ ----------
def build_evidences(con, rows, query: str, max_files: int, max_chars: int):
    """rankæ¸ˆã¿ rows ã‹ã‚‰ (dict) evidences ã‚’ä½œã‚‹"""
    evs = []
    for r in rows:
        joined = collect_until_next_heading_across_files(
            con, r["file"], r["start"], max_files=max_files, max_chars=max_chars
        )
        if not joined:
            continue
        level, title = parse_section(r["section"])
        score = section_rank(r["section"], query)
        key = f"{r['file']}|{r['section']}|{r['start']}"
        default_selected = (query.lower() in title.lower()) or (joined.lower().count(query.lower()) >= 1)
        evs.append({
            "key": key,
            "file": r["file"],
            "section": r["section"],
            "title": title,
            "level": level,
            "score": score,
            "text": joined,
            "selected": default_selected
        })
    # scoreé™é †ã§è¦‹ã‚„ã™ã
    evs.sort(key=lambda x: x["score"], reverse=True)
    return evs

def combined_text_from_selected(evs):
    parts = [e["text"] for e in evs if e["selected"]]
    return "\n\n------\n\n".join(parts)

# ---------- UI ----------
st.set_page_config(page_title="Markdown æ¤œç´¢ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ðŸ”Ž Markdown æ¤œç´¢ãƒ„ãƒ¼ãƒ«")

with st.sidebar:
    st.header("è¨­å®š")
    db_path = st.text_input("DBãƒ•ã‚¡ã‚¤ãƒ«", value=DB_PATH_DEFAULT)
    k = st.slider("Top-K", 1, 30, 10)
    mode = st.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰", "éƒ¨åˆ†ä¸€è‡´ï¼ˆLIKEï¼‰", "æ„å‘³æ¤œç´¢ï¼ˆEmbeddingï¼‰", "æŠ½å‡ºå›žç­”ï¼ˆRAGï¼‰", "å®Œå…¨ä¸€è‡´ï¼‹AIæ•´å½¢"])
    phrase = st.checkbox("ãƒ•ãƒ¬ãƒ¼ã‚ºå®Œå…¨ä¸€è‡´ï¼ˆâ€â€¦â€ï¼‰", value=False, disabled=(mode!="å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰"))

    # å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰ç”¨
    section_full = st.checkbox("ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰é …ç›®å…¨æ–‡ã‚’è¡¨ç¤ºï¼ˆ###è¦‹å‡ºã—å˜ä½ï¼‰", value=False, disabled=(mode!="å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰"))
    span_pages   = st.checkbox("ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰æ¬¡è¦‹å‡ºã—ã¾ã§ãƒ•ã‚¡ã‚¤ãƒ«æ¨ªæ–­ã§é€£çµ", value=True, disabled=(mode!="å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰"))
    max_files    = st.number_input("ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰é€£çµãƒšãƒ¼ã‚¸ä¸Šé™", min_value=1, value=10, step=1, disabled=(mode!="å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰"))
    max_chars    = st.number_input("ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰æœ€å¤§è¡¨ç¤ºæ–‡å­—æ•°ï¼ˆ0=ç„¡åˆ¶é™ï¼‰", min_value=0, value=0, step=1000, disabled=(mode!="å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰"))

    # AIæ•´å½¢ç”¨
    if mode == "å®Œå…¨ä¸€è‡´ï¼‹AIæ•´å½¢":
        ai_exact_k   = st.number_input("AIæ•´å½¢: å®Œå…¨ä¸€è‡´ã®Top-K", min_value=1, value=3, step=1)
        ai_max_files = st.number_input("AIæ•´å½¢: é€£çµãƒšãƒ¼ã‚¸ä¸Šé™", min_value=1, value=10, step=1)
        ai_max_chars = st.number_input("AIæ•´å½¢: æœ€å¤§æ–‡å­—æ•°ï¼ˆ0=ç„¡åˆ¶é™ï¼‰", min_value=0, value=8000, step=1000)
        ai_prompt    = st.text_area("AIæ•´å½¢ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "è£½é€ ä¼šç¤¾ãƒ»ç”¨é€”ãƒ»è¦åˆ¶ãƒ»å±é™ºæ€§ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚", height=120)
        ai_temp      = st.slider("AIæ¸©åº¦", 0.0, 1.0, 0.2, 0.1)
        save_flag    = st.checkbox("çµæžœã‚’Markdownã§ä¿å­˜", value=True)
        save_dir     = st.text_input("ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€", EXPORT_DIR)
        st.caption("âš ï¸ã€Žæ ¹æ‹ ä¸€è¦§ã€ã§ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã™ã¨ã€é™¤å¤–ã—ã¦å†è¦ç´„ã§ãã¾ã™ã€‚")

query = st.text_input("æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ä¸€è‡´ç”¨ï¼‰", value="", placeholder="ä¾‹ï¼šé…¢é…¸ãƒ–ãƒãƒ« / å…‰è§¦åª’é…¸åŒ–ãƒã‚¿ãƒ³ / ä¸»ãªæ²¹è„‚ã®æ§‹æˆè¡¨")
run = st.button("æ¤œç´¢ / å®Ÿè¡Œ")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
if "evidences" not in st.session_state:
    st.session_state.evidences = []
if "last_query" not in st.session_state:
    st.session_state.last_query = ""

def render_evidence_selector():
    st.subheader("æ ¹æ‹ ä¸€è¦§ï¼ˆäººæ‰‹ã§é¸æŠžå¯èƒ½ï¼‰")
    if not st.session_state.evidences:
        st.info("æ ¹æ‹ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    # è¡Œã”ã¨ã«ãƒã‚§ãƒƒã‚¯ï¼‹ãƒ¡ã‚¿æƒ…å ±ï¼‹æœ¬æ–‡
    for i, e in enumerate(st.session_state.evidences):
        cols = st.columns([0.1, 0.55, 0.15, 0.2])
        with cols[0]:
            st.session_state.evidences[i]["selected"] = st.checkbox(
                "", value=e["selected"], key=f"sel_{e['key']}")
        with cols[1]:
            st.markdown(f"**{e['title']}**  \n`{e['file']}`")
        with cols[2]:
            st.caption(f"å„ªå…ˆåº¦: {e['score']:.1f}  ï¼ˆéšŽå±¤ #{e['level']}ï¼‰")
        with cols[3]:
            st.caption(f"key: {e['key']}")
        with st.expander("åŽŸæ–‡ã‚’è¡¨ç¤º / äººé–“ã«ã‚ˆã‚‹ç¢ºèª", expanded=False):
            st.markdown(e["text"])
        st.divider()

if run and query.strip():
    con = get_conn(db_path)

    if mode == "å®Œå…¨ä¸€è‡´ï¼‹AIæ•´å½¢":
        # ã‚¯ã‚¨ãƒªãŒå¤‰ã‚ã£ãŸã‚‰ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚’ä½œã‚Šç›´ã—
        if st.session_state.last_query != query:
            st.session_state.evidences = []
            st.session_state.last_query = query

        rows = ranked_exact_hits(con, query.strip(), int(ai_exact_k), phrase)
        if not rows:
            st.warning("å®Œå…¨ä¸€è‡´ã®ãƒ’ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            evs = build_evidences(con, rows, query, int(ai_max_files), int(ai_max_chars))
            st.session_state.evidences = evs  # ä¿æŒ

            # 1) åˆå›žè¦ç´„ï¼ˆé¸æŠžçŠ¶æ…‹ã«å¾“ã†ï¼‰
            client = get_client()
            combined_text = combined_text_from_selected(st.session_state.evidences)
            if not combined_text:
                st.warning("é¸æŠžæ¸ˆã¿ã®æ ¹æ‹ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å°‘ãªãã¨ã‚‚1ã¤é¸æŠžã—ã¦ãã ã•ã„ã€‚")
            else:
                st.subheader("AIæ•´å½¢çµæžœ")
                md = ai_summarize_from_exact(client, query, combined_text, ai_prompt, ai_temp)
                st.markdown(md)

            # 2) äººæ‰‹ã§æ ¹æ‹ ã‚’ç¢ºèªãƒ»é¸æŠž
            render_evidence_selector()

            # 3) äººæ‰‹é¸æŠžã«åŸºã¥ãå†è¦ç´„
            if st.button("é¸æŠžã—ãŸæ ¹æ‹ ã®ã¿ã§å†è¦ç´„ã™ã‚‹"):
                combined_text2 = combined_text_from_selected(st.session_state.evidences)
                if not combined_text2:
                    st.warning("é¸æŠžæ¸ˆã¿ã®æ ¹æ‹ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å°‘ãªãã¨ã‚‚1ã¤é¸æŠžã—ã¦ãã ã•ã„ã€‚")
                else:
                    st.subheader("AIæ•´å½¢çµæžœï¼ˆå†è¦ç´„ï¼‰")
                    md = ai_summarize_from_exact(client, query, combined_text2, ai_prompt, ai_temp)
                    st.markdown(md)

            # 4) ä¿å­˜ï¼ˆé¸æŠžæ¸ˆã¿ã®æ ¹æ‹ ã®ã¿ï¼‰
            if save_flag:
                os.makedirs(save_dir, exist_ok=True)
                ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
                safe_q = re.sub(r"[^0-9A-Za-zä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ´ãƒ¼]", "_", query)[:50]
                path = os.path.join(save_dir, f"{ts}_{safe_q}_ai-format.md")
                with open(path, "w", encoding="utf-8") as f:
                    f.write("# AIæ•´å½¢çµæžœï¼ˆæœ€æ–°ã®è¡¨ç¤ºå†…å®¹ã‚’æ‰‹å‹•ã§ä¿å­˜ï¼‰\n\n")
                    # ç¾ç”»é¢ã«å‡ºã¦ã„ã‚‹æ•´å½¢çµæžœã¯ç›´ä¸Šã®å®Ÿè¡Œçµæžœã€‚å¿…è¦ã«å¿œã˜ã¦å†è¦ç´„å¾Œã«æŠ¼ã—ã¦ãã ã•ã„
                    f.write("(â€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯é¸æŠžæ¸ˆã¿æ ¹æ‹ ã«åŸºã¥ãæœ€æ–°ã®è¦ç´„ã‚’æ›¸ãå‡ºã™è¨­è¨ˆã§ã™ã€‚)\n\n")
                    # ç›´è¿‘ã®é¸æŠžçŠ¶æ…‹ã§å†ç”Ÿæˆã—ã¦ä¿å­˜
                    combined_text3 = combined_text_from_selected(st.session_state.evidences)
                    if combined_text3:
                        md3 = ai_summarize_from_exact(get_client(), query, combined_text3, ai_prompt, ai_temp)
                        f.write(md3 + "\n\n---\n\n")
                    f.write(f"## ãƒ¡ã‚¿æƒ…å ±\n- æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰: {query}\n- ãƒ¢ãƒ‡ãƒ«: {ANSWER_MODEL}\n- ä¿å­˜æ—¥æ™‚: {ts}\n\n")
                    f.write("## æ ¹æ‹ ï¼ˆé¸æŠžæ¸ˆã¿ã®ã¿ï¼‰\n")
                    for e in st.session_state.evidences:
                        if not e["selected"]:
                            continue
                        f.write(f"### {e['title']}  ({e['file']})\n\n{e['text']}\n\n---\n\n")
                st.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {path}")

    elif mode == "å®Œå…¨ä¸€è‡´ï¼ˆFTSï¼‰":
        rows = ranked_exact_hits(con, query.strip(), int(k), phrase)
        st.subheader(f"å®Œå…¨ä¸€è‡´ï¼ˆ{len(rows)}ä»¶ï¼‰")
        for r in rows:
            st.markdown(f"**[{r['file']}] {r['section']}**")
            if span_pages:
                joined = collect_until_next_heading_across_files(
                    con, r["file"], r["start"], max_files=int(max_files), max_chars=int(max_chars)
                )
                st.markdown(joined if joined else r["snippet"])
            elif section_full:
                full = fetch_section_text(con, r["file"], r["section"])
                if int(max_chars) and len(full) > int(max_chars):
                    full = full[:int(max_chars)] + " â€¦"
                st.markdown(full if full else r["snippet"])
            else:
                st.markdown(r["snippet"])
            st.divider()

    elif mode == "éƒ¨åˆ†ä¸€è‡´ï¼ˆLIKEï¼‰":
        rows = substring_like(con, query.strip(), int(k))
        st.subheader(f"éƒ¨åˆ†ä¸€è‡´ï¼ˆ{len(rows)}ä»¶ï¼‰")
        for r in rows:
            st.markdown(f"**[{r['file']}] {r['section']}**  \n{r['snippet']}")
            st.divider()

    elif mode == "æ„å‘³æ¤œç´¢ï¼ˆEmbeddingï¼‰":
        vecs, ids = fetch_vectors(con)
        if vecs.shape[0] == 0:
            st.error("ãƒ™ã‚¯ãƒˆãƒ«ãŒæœªä½œæˆã§ã™ã€‚å…ˆã« CLI ã§ `index --embed` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            client = get_client()
            qv = embed(client, [query])[0]
            sims = vecs @ qv / (np.linalg.norm(vecs,axis=1)*np.linalg.norm(qv) + 1e-9)
            top = np.argsort(-sims)[:int(k)]
            sel_ids = [ids[i] for i in top]
            rows = fetch_passages_by_ids(con, sel_ids)
            st.subheader(f"æ„å‘³æ¤œç´¢ï¼ˆ{len(rows)}ä»¶ï¼‰")
            for r,score in zip(rows, sims[top]):
                preview = r["text"].replace("\n"," ")
                if len(preview) > 180: preview = preview[:180]+" â€¦"
                st.markdown(f"**[{r['file']}] {r['section']}**  \nscore={score:.3f}  \n{preview}")
                st.divider()

    else:  # æŠ½å‡ºå›žç­”ï¼ˆRAGï¼‰ç°¡æ˜“
        st.info("RAGã¯ç°¡æ˜“è¡¨ç¤ºã§ã™ã€‚ã¾ãšã€Žå®Œå…¨ä¸€è‡´ï¼‹AIæ•´å½¢ã€ã§äººæ‰‹ç¢ºèªâ†’å†è¦ç´„ã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚")
        vecs, ids = fetch_vectors(con)
        if vecs.shape[0] == 0:
            st.error("ãƒ™ã‚¯ãƒˆãƒ«ãŒæœªä½œæˆã§ã™ã€‚å…ˆã« CLI ã§ `index --embed` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            client = get_client()
            qv = embed(client, [query])[0]
            sims = vecs @ qv / (np.linalg.norm(vecs,axis=1)*np.linalg.norm(qv) + 1e-9)
            top = np.argsort(-sims)[:int(k)]
            sel_ids = [ids[i] for i in top]
            rows = fetch_passages_by_ids(con, sel_ids)
            st.subheader(f"RAGå€™è£œï¼ˆæ„å‘³æ¤œç´¢ Top-Kï¼‰")
            for r,score in zip(rows, sims[top]):
                st.markdown(f"> **[id:{r['id']} | {r['file']} | {r['section']}]** (score={score:.3f})\n\n{r['text']}")
                st.divider()
